# Проблема с загрузкой json

Основная метрика - время обработки и загрузки в бд данных из json файла.
Выделил код для загрузки в сервис (просто для удобства), написал тест на регресс, и на быстродействие.
Тест использует данные небольшого размера, чтобы тесты проходили не очень долго, и прогресс быстродействия был измерим.
В первоначальном виде скрипт, загружающий данные, работает не более 20 секунд.
Это значение (20 сек) было выставлено в качестве нижнего порога для теста быстродействия.

Можно переходить к оптимизации.

#### Находка 1
- потратил некоторое время на профилирование скрипта с помощью ruby-prof. Но столкнулся с тем, что работа скрипта сильно замедлилась после включения профилировщика, и даже при обработке small.json я не мог дождаться окончания работы скрипта.
После уменьшения объема входных данных отчеты ruby-prof ничего интересного не показали: всю основную работу выполняет Active Record, при чем все последующие операции вложены друг в друга (я так понял так делается для возможности обеспечения транзакционности).
По отчетам я не смог точно определить, на какую именно операцию AR тратит больше всего времени, но видимо это частый коннект к базе данных, множество запросов поиска и вставки.
Было решено использовать гем active-record-import для оптимизации количества обращений к бд.
- переписал скрипт так, чтобы при обходе файла собирались массивы с объектами городов, автобусов, сервисов, трипов. После окончания обхода файла подгружаю поочередно каждую сущность с помощью гема activerecord-import.
- удалось уменьшить время выполнения скрипта для small.json с 20 секунд до 2 сек

#### Находка 2
- при запуске скрипта на large.json процесс убивается через некоторое время не отработав до конца.
Отчеты ruby-prof в режиме памяти показали, что слабым местом является вызовы Class.new.
- вместо создания объектов соответствующих моделей, сделал хеши.
- удалось уменьшить время выполнения скрипта для small.json с 2 секунд до 800 мс.
Теперь скрипт справляется с large.json и укладывается в 60 секунд.